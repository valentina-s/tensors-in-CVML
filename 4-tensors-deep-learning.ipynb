{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two separate notions:\n",
    "\n",
    "* data as tensors\n",
    "* the transformations of the data as tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations in Deep Neural Nets:\n",
    "    \n",
    "* fully connected layers\n",
    "* convolutional layers\n",
    "* activation functions\n",
    "* pooling operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet Architecture '12\n",
    "![](img/alexnet2.png)\n",
    "\n",
    "[Image Source](https://world4jason.gitbooks.io/research-log/content/deepLearning/CNN/Model%20&%20ImgNet/alexnet/alexnet.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main uses of tensors in deep learning:\n",
    "\n",
    "* can speed up processing - compression \n",
    "* can obtain theoretical properties by analyzing tensor representations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compression:\n",
    "\n",
    "[*Tensorizing Neural Networks*](https://arxiv.org/pdf/1509.06569.pdf), Novikov, et. al., 2015\n",
    "* fully connected layers are linear by default, but converting them to a tensor-train format can largely reduce the network size\n",
    "\n",
    "Tensor-Train Format:\n",
    "![](img/tensor-train.png)\n",
    "[Image Source](https://www.slideshare.net/AlexanderNovikov8/tensor-train-decomposition-in-machine-learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Universility:\n",
    "[*Convolutional Rectifier Networks as Generalized Tensor Decompositions*](https://arxiv.org/pdf/1603.00162.pdf),  Cohen & Shashua, 2016\n",
    "\n",
    "* the transformation of a shallow convolutional network with ReLU activation functions is equivalent to generalized CP decomposition\n",
    "* the transformation of a deep convolutional network with ReLU activation functions is equivalent to generalized Hierarchical Tucker decomposition\n",
    "* ReLU convnets with max and product pooling are universal\n",
    "* ReLU convnets with average pooling are not universal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Optimality:\n",
    "[Global Optimality in Tensor Factorization, Deep Learning, and Beyond](https://arxiv.org/abs/1506.07540), Haeffele & Vidal, 2015\n",
    "* local minimizers are global minimizers\n",
    "* local descent can reach the global minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
